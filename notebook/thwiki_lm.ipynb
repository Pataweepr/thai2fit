{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai2fit Language Model Pre-training\n",
    "\n",
    "The goal of this notebook is to train a language model using the [fast.ai](http://www.fast.ai/) version of [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182), with data from [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) last updated February 17, 2019. Using 40M/200k/200k tokens of train-validation-test split, we achieved validation perplexity of **46.80959 with 60,002 embeddings at 400 dimensions**, compared to state-of-the-art as of October 27, 2018 at **42.41 for English WikiText-2 by [Yang et al (2018)](https://arxiv.org/abs/1711.03953)**. To the best of our knowledge, there is no comparable research in Thai language at the point of writing (February 17, 2019).\n",
    "\n",
    "Our workflow is as follows:\n",
    "\n",
    "* Retrieve and process [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) according to [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)\n",
    "* Perform 40M/200k/200k tokens of train-validation-test split split\n",
    "* Minimal text cleaning and tokenization using `newmm` with frozen dictionary (`engine='ulmfit'`) of [pyThaiNLP](https://github.com/pyThaiNLP/pythainlp/)\n",
    "* Train language model\n",
    "* Evaluate model based on perplexity and eyeballing\n",
    "* Extract embeddings to use as \"word2vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:30:25.407566Z",
     "start_time": "2018-01-25T03:30:21.597641Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *    \n",
    "from fastai.text import * \n",
    "from utils import *\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "data_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the dataset creation, pre- and post-processing of [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual):\n",
    "\n",
    "* `ulmfit/create_wikitext.py` - Download thwiki in json format and separate them into 40M/200k/200k tokens of train-validation-test split. Also perform tokenization with whitespaces as separators.\n",
    "* `ulmfit/postprocess_wikitext.py` - Replace numbers and replace out-of-vocabulary tokens with `xxunk` (frequency of less than 3).\n",
    "\n",
    "We replaced the Moses Tokenizer with the following code to use [pyThaiNLP](https://github.com/pyThaiNLP/pythainlp/)'s `newmm` dictionary-based tokenizer with a frozen dictionary instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "class ThaiNLPTokenizer:\n",
    "    def __init__(self,engine='ulmfit'):\n",
    "        self.engine='ulmfit'\n",
    "    def tokenize(self, t, return_str=True):\n",
    "        res = word_tokenize(t,self.engine)\n",
    "        return ' '.join(res) if return_str else res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the statistics of the dataset:\n",
    "\n",
    "```\n",
    "before postprocessing\n",
    "# documents: 121,027. # tokens: 39,378,410\n",
    "\n",
    "after postprocessing\n",
    "OOV ratio: 0.0042\n",
    "data/wiki/th-all vocab size: 111,224\n",
    "th.wiki.train.tokens. # of tokens: 41,482,435\n",
    "th.wiki.valid.tokens. # of tokens: 200,563\n",
    "th.wiki.test.tokens. # of tokens: 200,827\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We used the `newmm` engine of `pyThaiNLP` to perform tokenization. Out of randomnum tokens from all of training set, we chose 60,000 embeddings (plus two for unknown and padding) of tokens which appeared more than twice (not typos) in the training set.\n",
    "\n",
    "\n",
    "We perform the following text processing:\n",
    "\n",
    "* Fix html tags to plain texts\n",
    "* Lowercase all English words and if a word is written in all caps, we put it in a lower case and add `xxup` before\n",
    "* Repetitive characters: Thai usually emphasizes adjectives by repeating the last character such as `อร่อยมากกกกกกก` to `อร่อยมาก xxrep 7 ` so that the word still retains its original form. \n",
    "* Normalize character order: for instance `นำ้` to `น้ำ`\n",
    "* Add spaces around / and #\n",
    "* Remove multiple spaces and newlines\n",
    "* Remove empty brackets of all types (`([{`) which might result from cleaning up\n",
    "* `pyThaiNLP`'s `newmm` word tokenizer with frozen dictionary (`engine ='ulmfit'`)  is used to tokenize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thai Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `newmm` tokenizer with a dictionary frozen as of 2018-10-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:50:02.035245Z",
     "start_time": "2018-01-24T15:50:01.940191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['วิทยาศาสตร์',\n",
       " 'ดาวเคราะห์',\n",
       " 'เป็น',\n",
       " 'สาขาวิชา',\n",
       " 'ที่',\n",
       " 'ศึกษา',\n",
       " 'เกี่ยวกับ',\n",
       " 'องค์ประกอบ',\n",
       " 'ของ',\n",
       " 'ดาวเคราะห์']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='วิทยาศาสตร์ดาวเคราะห์เป็นสาขาวิชาที่ศึกษาเกี่ยวกับองค์ประกอบของดาวเคราะห์'\n",
    "a = word_tokenize(text,engine='ulmfit')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrated into pythainlp.ulmfit.utils\n",
    "from fastai.text.transform import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import normalize as normalize_char_order\n",
    "\n",
    "class ThaiTokenizer(BaseTokenizer):\n",
    "    \"Wrapper around a newmm tokenizer to make it a `BaseTokenizer`.\"\n",
    "    def __init__(self, lang:str = 'th'):\n",
    "        self.lang = lang\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        return(word_tokenize(t,engine='ulmfit'))\n",
    "    def add_special_cases(self, toks:Collection[str]):\n",
    "        pass\n",
    "    \n",
    "def replace_rep_after(t:str) -> str:\n",
    "    \"Replace repetitions at the character level in `t` after the repetition\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {c} {TK_REP} {len(cc)+1} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "def rm_useless_newlines(t:str) -> str:\n",
    "    \"Remove multiple newlines in `t`.\"\n",
    "    return re.sub('[\\n]{2,}', ' ', t)\n",
    "\n",
    "def rm_brackets(t:str) -> str:\n",
    "    \"Remove all empty brackets from `t`.\"\n",
    "    new_line = re.sub('\\(\\)','',t)\n",
    "    new_line = re.sub('\\{\\}','',new_line)\n",
    "    new_line = re.sub('\\[\\]','',new_line)\n",
    "    return(new_line)\n",
    "\n",
    "#in case we want to add more specific rules for thai\n",
    "thai_rules = [fix_html, deal_caps, replace_rep_after, normalize_char_order, \n",
    "              spec_add_spaces, rm_useless_spaces, rm_useless_newlines, rm_brackets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the language model based on 80/20 train-validation split from Thai Wikipedia. Tokens are generated and numericalized filtering all words with frequency more than 2 and at maximum vocab size of 60,000 (plus unknown and padding tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', rules = thai_rules)\n",
    "# data_lm = TextLMDataBunch.from_csv(path = Path(DATA_PATH),csv_name='train.csv',valid_pct=0.01,\n",
    "#                                   tokenizer=tt, min_freq=2, max_vocab=60000, bs=32)\n",
    "# data.save('databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLMDataBunch.load(DATA_PATH,'qrnn_db',bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20706, 2049894)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.valid_ds), len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,  5941,  7611,  ...,     2,   732,    11],\n",
       "        [    2,    42,  4357,  ...,    41,     8, 38718],\n",
       "        [    3, 11114,  3112,  ...,    19,  1125,    11],\n",
       "        ...,\n",
       "        [ 1725,  6629, 48984,  ...,  8315,     2, 10855],\n",
       "        [   38, 21976,    52,  ...,   647,    60,  2388],\n",
       "        [  348,     2,     2,  ...,   143,    72,     0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data.valid_dl))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10986, 7391, 1405]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm = data.vocab\n",
    "vocab_lm.numericalize(word_tokenize('สวัสดีครับพี่น้อง', engine='ulmfit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดี ครับ พี่น้อง'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm.textify([10986, 7391, 1405])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "We train the language model according to the [ULMFit paper](https://arxiv.org/abs/1801.06146) but replacing LSTM modules with [QRNN modules](https://arxiv.org/abs/1611.01576)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select weight decays and lrs\n",
    "# wds = [1e-8,1e-7,1e-6]\n",
    "# plts = []\n",
    "# for wd in wds:\n",
    "#     learn = language_model_learner(data, bptt = 70, emb_sz = 300, nh = 1550, nl = 3,\n",
    "#                                   drop_mult = 0.1, bias = True, qrnn = True, tie_weights=True,\n",
    "#                                   pretrained_fnames = None)\n",
    "#     learn.wd = wd\n",
    "#     learn.opt_func = partial(optim.Adam, betas=(0.8, 0.99)) #heuristic reference from imdb_scripts\n",
    "#     learn.lr_find(start_lr = 1e-3, end_lr = 1e1)\n",
    "#     plts.append(learn.recorder.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with 20% Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heuristic reference from imdb_scripts\n",
    "learn = language_model_learner(data, bptt = 70, emb_sz = 300, nh = 1550, nl = 3,\n",
    "                                  drop_mult = 0.1, bias = True, qrnn = True, \n",
    "                                  alpha=2, beta = 1, clip = 0.12, tie_weights=True,\n",
    "                                  pretrained_fnames = None)\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_func = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "wd = 1e-7\n",
    "learn.wd=wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(cyc_len = 20, \n",
    "#                     max_lr= lr, #learning rate\n",
    "#                     div_factor=20, #factor to discount from max\n",
    "#                     moms = (0.8, 0.7), #momentums\n",
    "#                     pct_start = 0.1, #where the peak is at \n",
    "#                     wd = wd #weight decay\n",
    "#                    ) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch  train_loss  valid_loss  accuracy\n",
    "1      4.154932    4.285707    0.321085\n",
    "2      4.221297    4.275705    0.320524\n",
    "3      4.096167    4.186819    0.329127\n",
    "4      4.074309    4.129424    0.334466\n",
    "5      4.007462    4.091810    0.338730\n",
    "6      4.016120    4.059826    0.341838\n",
    "7      3.998191    4.037315    0.344132\n",
    "8      3.969566    4.012897    0.346576\n",
    "9      3.980394    3.985680    0.349456\n",
    "10     3.935588    3.974943    0.351947\n",
    "11     3.922036    3.944212    0.354973\n",
    "12     3.896322    3.924284    0.356794\n",
    "13     3.843060    3.900109    0.359668\n",
    "14     3.865328    3.884382    0.362396\n",
    "15     3.859496    3.868790    0.364474\n",
    "16     3.824713    3.855656    0.366229\n",
    "17     3.764611    3.843762    0.367919\n",
    "18     3.826664    3.836620    0.368990\n",
    "19     3.835252    3.831286    0.369607\n",
    "20     3.827917    3.829568    0.369697"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training with 20% Validation and Less Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tried to train it a little more with lower learnng rates, dropouts and weight decays but not really helpful\n",
    "learn = language_model_learner(data, bptt = 70, emb_sz = 400, nh = 1550, nl = 3,\n",
    "                                  drop_mult = 0., bias = True, qrnn = True, \n",
    "                                  alpha=2, beta = 1, clip = 0.12, tie_weights=True,\n",
    "                                  pretrained_fnames = None)\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_func = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "wd = 1e-8\n",
    "learn.wd=wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(cyc_len = 10, \n",
    "#                     max_lr= lr, #learning rate\n",
    "#                     div_factor=20, #factor to discount from max\n",
    "#                     moms = (0.8, 0.7), #momentums\n",
    "#                     pct_start = 0.1, #where the peak is at \n",
    "#                     wd = wd #weight decay\n",
    "#                    ) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch  train_loss  valid_loss  accuracy\n",
    "1      3.714261    3.895127    0.362198\n",
    "2      3.705848    3.892921    0.362874\n",
    "3      3.684441    3.888398    0.363410\n",
    "4      3.767845    3.882282    0.364280\n",
    "5      3.764945    3.872842    0.365416\n",
    "6      3.643065    3.866705    0.366921\n",
    "7      3.671365    3.860282    0.368246\n",
    "8      3.605198    3.854719    0.369281\n",
    "9      3.540176    3.849211    0.370150\n",
    "10     3.616597    3.846088    0.370464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with 1% Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch  train_loss  valid_loss  accuracy\n",
    "1      3.251031    3.288332    0.471518\n",
    "2      3.304003    3.313594    0.464731\n",
    "3      3.268891    3.269042    0.468432\n",
    "4      3.328306    3.254568    0.468563\n",
    "5      3.268297    3.261068    0.467811\n",
    "6      3.318623    3.255545    0.469078\n",
    "7      3.306940    3.257410    0.469546\n",
    "8      3.235727    3.238553    0.471659\n",
    "9      3.332875    3.237368    0.471804\n",
    "10     3.314787    3.227757    0.472428\n",
    "11     3.297205    3.228444    0.472566\n",
    "12     3.299368    3.210011    0.474784\n",
    "13     3.335267    3.198320    0.475990\n",
    "14     3.251518    3.191868    0.477182\n",
    "15     3.232381    3.189055    0.477916\n",
    "16     3.257015    3.189897    0.478915\n",
    "17     3.242529    3.192807    0.479459\n",
    "18     3.299369    3.199118    0.479859\n",
    "19     3.280200    3.197080    0.480416\n",
    "20     3.227655    3.196661    0.480281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeballing Test\n",
    "We perform eyeballing test by having the model \"fill in the blanks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, bptt = 70, emb_sz = 400, nh = 1550, nl = 3,\n",
    "                                  drop_mult = 0., bias = True, qrnn = True, \n",
    "                                  alpha=2, beta = 1, clip = 0.12,\n",
    "                                  pretrained_fnames = None)\n",
    "learn.load('thwiki_model_qrnn')\n",
    "m = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_next(ss,topk):\n",
    "    s = word_tokenize(ss,engine='ulmfit')\n",
    "    t = torch.LongTensor(data.train_ds.vocab.numericalize(s)).view(-1,1).to(device)\n",
    "    t.requires_grad = False\n",
    "    m.reset()\n",
    "    pred,*_ = m(t)\n",
    "    pred_i = pred[-1].topk(topk)[1]\n",
    "    return(data.train_ds.vocab.textify(pred_i))\n",
    "\n",
    "def gen_sentences(ss,nb_words):\n",
    "    result = []\n",
    "    s = word_tokenize(ss,engine='ulmfit')\n",
    "    t = torch.LongTensor(data.train_ds.vocab.numericalize(s)).view(-1,1).to(device)\n",
    "    t.requires_grad = False\n",
    "    m.reset()\n",
    "    pred,*_ = m(t)\n",
    "    for i in range(nb_words):\n",
    "        pred_i = pred[-1].topk(2)[1]\n",
    "        #get first one if not unknowns, pads, or spaces\n",
    "        pred_i = pred_i[1] if pred_i.data[0] == 0 else pred_i[0]\n",
    "        pred_i = pred_i.view(-1,1)\n",
    "        result.append(data.train_ds.vocab.textify(pred_i))\n",
    "        t = torch.cat((t,pred_i))\n",
    "        pred,*_ = m(t)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ข้างนอก ข้างหลัง ทุกที่ เบื้องล่าง เบื้องหน้า ถึงกัน สะดุด ตามรอย ที่ใด ข้างบน'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'ฉันเดินไป'\n",
    "gen_next(ss,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ข้างนอกข้างนอก ฉันจะเดินต่อไป และเดินต่อไป เราจะนั่งพักผ่อน เราจะเดินถอยหลัง เราจะเราเราจะนั่งพักผ่อนฉัน เราจะนั่งบนเก้าอี้ เราเราจะนั่งเดินบนเก้าอี้ เราจะนั่งตรงหน้าฉันนั่งบนเก้าอี้ เราจะนั่งเดินบนรันเวย์ เราจะนั่งเดินบนเก้าอี้ เราจะนั่งอาสน์ เราจะนั่งเดินบนเก้าอี้ เราจะเดินต่อไป เราจะนั่งบนเก้าอี้ เราจะนั่งเดินบนรันเวย์ เราจะ'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'ฉันเดินไป'\n",
    "''.join(gen_sentences(ss,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We extract the embedding layer of the encoder to be used in the same manner as `word2vec`. We can also create sentence vector by summing or averaging the vectors. For more details about `word2vec` use cases, see`word2vec_examples.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:14:19.257675Z",
     "start_time": "2018-01-24T17:14:19.219043Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_weights = list(learn.model.named_parameters())[0][1]\n",
    "emb_np = to_np(emb_weights.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:55:01.593405Z",
     "start_time": "2018-01-25T03:55:01.533652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xxunk</th>\n",
       "      <td>9.412697</td>\n",
       "      <td>-0.488594</td>\n",
       "      <td>-3.401881</td>\n",
       "      <td>-42.730244</td>\n",
       "      <td>35.064205</td>\n",
       "      <td>-26.100941</td>\n",
       "      <td>-3.747117</td>\n",
       "      <td>15.602914</td>\n",
       "      <td>37.966507</td>\n",
       "      <td>1.014693</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.495045</td>\n",
       "      <td>-25.819569</td>\n",
       "      <td>38.371155</td>\n",
       "      <td>6.613634</td>\n",
       "      <td>1.742496</td>\n",
       "      <td>-9.713803</td>\n",
       "      <td>-12.573798</td>\n",
       "      <td>1.576579</td>\n",
       "      <td>-20.847286</td>\n",
       "      <td>-1.370693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxpad</th>\n",
       "      <td>3.876736</td>\n",
       "      <td>-0.049269</td>\n",
       "      <td>-0.021756</td>\n",
       "      <td>-33.481445</td>\n",
       "      <td>31.611290</td>\n",
       "      <td>-20.919840</td>\n",
       "      <td>-0.508237</td>\n",
       "      <td>14.074152</td>\n",
       "      <td>32.848042</td>\n",
       "      <td>-1.403501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038427</td>\n",
       "      <td>-19.635546</td>\n",
       "      <td>29.998386</td>\n",
       "      <td>7.486456</td>\n",
       "      <td>0.269155</td>\n",
       "      <td>-11.942196</td>\n",
       "      <td>-4.076655</td>\n",
       "      <td>-0.319159</td>\n",
       "      <td>-13.353608</td>\n",
       "      <td>-0.664269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxspace</th>\n",
       "      <td>10.631902</td>\n",
       "      <td>-3.973876</td>\n",
       "      <td>-1.743042</td>\n",
       "      <td>-42.599854</td>\n",
       "      <td>34.616451</td>\n",
       "      <td>-26.990564</td>\n",
       "      <td>10.954692</td>\n",
       "      <td>14.398766</td>\n",
       "      <td>37.990967</td>\n",
       "      <td>-3.763945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.278642</td>\n",
       "      <td>-25.836325</td>\n",
       "      <td>40.264915</td>\n",
       "      <td>7.057155</td>\n",
       "      <td>1.878293</td>\n",
       "      <td>-9.766123</td>\n",
       "      <td>-11.934086</td>\n",
       "      <td>0.281595</td>\n",
       "      <td>-21.392384</td>\n",
       "      <td>-2.502031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.687781</td>\n",
       "      <td>-3.374289</td>\n",
       "      <td>-5.566193</td>\n",
       "      <td>-41.561626</td>\n",
       "      <td>36.881172</td>\n",
       "      <td>-28.293665</td>\n",
       "      <td>-1.500996</td>\n",
       "      <td>15.841971</td>\n",
       "      <td>39.105003</td>\n",
       "      <td>-0.694619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800407</td>\n",
       "      <td>-26.646166</td>\n",
       "      <td>42.336922</td>\n",
       "      <td>7.244391</td>\n",
       "      <td>1.356798</td>\n",
       "      <td>-10.226693</td>\n",
       "      <td>-12.592010</td>\n",
       "      <td>-1.624527</td>\n",
       "      <td>-21.064072</td>\n",
       "      <td>-9.900548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxeol</th>\n",
       "      <td>10.076826</td>\n",
       "      <td>-3.298112</td>\n",
       "      <td>-3.062871</td>\n",
       "      <td>-42.180008</td>\n",
       "      <td>34.165661</td>\n",
       "      <td>-27.856457</td>\n",
       "      <td>6.696773</td>\n",
       "      <td>15.293684</td>\n",
       "      <td>38.029015</td>\n",
       "      <td>-4.269083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678904</td>\n",
       "      <td>-25.555805</td>\n",
       "      <td>40.705482</td>\n",
       "      <td>7.259528</td>\n",
       "      <td>1.353030</td>\n",
       "      <td>-14.982669</td>\n",
       "      <td>-13.479355</td>\n",
       "      <td>-0.881967</td>\n",
       "      <td>-20.467594</td>\n",
       "      <td>-5.054183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xxfld</th>\n",
       "      <td>14.899630</td>\n",
       "      <td>-1.930843</td>\n",
       "      <td>2.001746</td>\n",
       "      <td>-42.514164</td>\n",
       "      <td>38.589172</td>\n",
       "      <td>-29.482353</td>\n",
       "      <td>-3.962751</td>\n",
       "      <td>3.097306</td>\n",
       "      <td>39.768669</td>\n",
       "      <td>-0.693229</td>\n",
       "      <td>...</td>\n",
       "      <td>3.201032</td>\n",
       "      <td>-22.658134</td>\n",
       "      <td>44.450848</td>\n",
       "      <td>6.989926</td>\n",
       "      <td>-1.499702</td>\n",
       "      <td>-7.791940</td>\n",
       "      <td>-4.544464</td>\n",
       "      <td>0.243512</td>\n",
       "      <td>-23.966330</td>\n",
       "      <td>-6.069082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ใน</th>\n",
       "      <td>9.683018</td>\n",
       "      <td>-1.806045</td>\n",
       "      <td>-3.538491</td>\n",
       "      <td>-42.151825</td>\n",
       "      <td>34.464622</td>\n",
       "      <td>-28.008196</td>\n",
       "      <td>-2.961989</td>\n",
       "      <td>14.223104</td>\n",
       "      <td>37.152637</td>\n",
       "      <td>1.243370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138160</td>\n",
       "      <td>-26.050760</td>\n",
       "      <td>38.420036</td>\n",
       "      <td>7.372793</td>\n",
       "      <td>2.484624</td>\n",
       "      <td>-11.345880</td>\n",
       "      <td>-17.197050</td>\n",
       "      <td>-0.729840</td>\n",
       "      <td>-20.466942</td>\n",
       "      <td>-0.287305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ที่</th>\n",
       "      <td>9.770124</td>\n",
       "      <td>-4.167158</td>\n",
       "      <td>-0.651670</td>\n",
       "      <td>-42.355728</td>\n",
       "      <td>36.287140</td>\n",
       "      <td>-26.348225</td>\n",
       "      <td>-2.798183</td>\n",
       "      <td>15.309365</td>\n",
       "      <td>37.787487</td>\n",
       "      <td>-1.505493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>-26.294212</td>\n",
       "      <td>39.143227</td>\n",
       "      <td>7.409772</td>\n",
       "      <td>1.725253</td>\n",
       "      <td>-10.572651</td>\n",
       "      <td>-18.086336</td>\n",
       "      <td>-0.444782</td>\n",
       "      <td>-21.733566</td>\n",
       "      <td>-4.995567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>และ</th>\n",
       "      <td>9.779366</td>\n",
       "      <td>-4.125625</td>\n",
       "      <td>-5.410614</td>\n",
       "      <td>-42.443588</td>\n",
       "      <td>35.464420</td>\n",
       "      <td>-26.667896</td>\n",
       "      <td>2.241813</td>\n",
       "      <td>13.938269</td>\n",
       "      <td>37.312401</td>\n",
       "      <td>-4.813928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503761</td>\n",
       "      <td>-25.952402</td>\n",
       "      <td>38.727116</td>\n",
       "      <td>7.179166</td>\n",
       "      <td>2.370239</td>\n",
       "      <td>-8.047087</td>\n",
       "      <td>-15.263973</td>\n",
       "      <td>2.641281</td>\n",
       "      <td>-21.697216</td>\n",
       "      <td>-2.367746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>เป็น</th>\n",
       "      <td>8.904553</td>\n",
       "      <td>-3.972028</td>\n",
       "      <td>0.898944</td>\n",
       "      <td>-41.665886</td>\n",
       "      <td>35.541092</td>\n",
       "      <td>-28.023504</td>\n",
       "      <td>-3.800491</td>\n",
       "      <td>16.072670</td>\n",
       "      <td>38.580750</td>\n",
       "      <td>-2.286343</td>\n",
       "      <td>...</td>\n",
       "      <td>2.570196</td>\n",
       "      <td>-25.606272</td>\n",
       "      <td>39.234039</td>\n",
       "      <td>7.785551</td>\n",
       "      <td>1.330086</td>\n",
       "      <td>-12.832536</td>\n",
       "      <td>-14.681183</td>\n",
       "      <td>-1.536232</td>\n",
       "      <td>-21.056726</td>\n",
       "      <td>-2.545995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2          3          4          5    \\\n",
       "xxunk     9.412697 -0.488594 -3.401881 -42.730244  35.064205 -26.100941   \n",
       "xxpad     3.876736 -0.049269 -0.021756 -33.481445  31.611290 -20.919840   \n",
       "xxspace  10.631902 -3.973876 -1.743042 -42.599854  34.616451 -26.990564   \n",
       "1         9.687781 -3.374289 -5.566193 -41.561626  36.881172 -28.293665   \n",
       "xxeol    10.076826 -3.298112 -3.062871 -42.180008  34.165661 -27.856457   \n",
       "xxfld    14.899630 -1.930843  2.001746 -42.514164  38.589172 -29.482353   \n",
       "ใน        9.683018 -1.806045 -3.538491 -42.151825  34.464622 -28.008196   \n",
       "ที่       9.770124 -4.167158 -0.651670 -42.355728  36.287140 -26.348225   \n",
       "และ       9.779366 -4.125625 -5.410614 -42.443588  35.464420 -26.667896   \n",
       "เป็น      8.904553 -3.972028  0.898944 -41.665886  35.541092 -28.023504   \n",
       "\n",
       "               6          7          8         9      ...          390  \\\n",
       "xxunk    -3.747117  15.602914  37.966507  1.014693    ...    -6.495045   \n",
       "xxpad    -0.508237  14.074152  32.848042 -1.403501    ...     1.038427   \n",
       "xxspace  10.954692  14.398766  37.990967 -3.763945    ...    -1.278642   \n",
       "1        -1.500996  15.841971  39.105003 -0.694619    ...     0.800407   \n",
       "xxeol     6.696773  15.293684  38.029015 -4.269083    ...    -0.678904   \n",
       "xxfld    -3.962751   3.097306  39.768669 -0.693229    ...     3.201032   \n",
       "ใน       -2.961989  14.223104  37.152637  1.243370    ...     0.138160   \n",
       "ที่      -2.798183  15.309365  37.787487 -1.505493    ...    -0.002396   \n",
       "และ       2.241813  13.938269  37.312401 -4.813928    ...    -0.503761   \n",
       "เป็น     -3.800491  16.072670  38.580750 -2.286343    ...     2.570196   \n",
       "\n",
       "               391        392       393       394        395        396  \\\n",
       "xxunk   -25.819569  38.371155  6.613634  1.742496  -9.713803 -12.573798   \n",
       "xxpad   -19.635546  29.998386  7.486456  0.269155 -11.942196  -4.076655   \n",
       "xxspace -25.836325  40.264915  7.057155  1.878293  -9.766123 -11.934086   \n",
       "1       -26.646166  42.336922  7.244391  1.356798 -10.226693 -12.592010   \n",
       "xxeol   -25.555805  40.705482  7.259528  1.353030 -14.982669 -13.479355   \n",
       "xxfld   -22.658134  44.450848  6.989926 -1.499702  -7.791940  -4.544464   \n",
       "ใน      -26.050760  38.420036  7.372793  2.484624 -11.345880 -17.197050   \n",
       "ที่     -26.294212  39.143227  7.409772  1.725253 -10.572651 -18.086336   \n",
       "และ     -25.952402  38.727116  7.179166  2.370239  -8.047087 -15.263973   \n",
       "เป็น    -25.606272  39.234039  7.785551  1.330086 -12.832536 -14.681183   \n",
       "\n",
       "              397        398       399  \n",
       "xxunk    1.576579 -20.847286 -1.370693  \n",
       "xxpad   -0.319159 -13.353608 -0.664269  \n",
       "xxspace  0.281595 -21.392384 -2.502031  \n",
       "1       -1.624527 -21.064072 -9.900548  \n",
       "xxeol   -0.881967 -20.467594 -5.054183  \n",
       "xxfld    0.243512 -23.966330 -6.069082  \n",
       "ใน      -0.729840 -20.466942 -0.287305  \n",
       "ที่     -0.444782 -21.733566 -4.995567  \n",
       "และ      2.641281 -21.697216 -2.367746  \n",
       "เป็น    -1.536232 -21.056726 -2.545995  \n",
       "\n",
       "[10 rows x 400 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai2vec = pd.DataFrame(emb_np)\n",
    "new_itos = data.vocab.itos\n",
    "#replace space with xxspace\n",
    "new_itos[2] = 'xxspace'\n",
    "#replace space for named entities with _\n",
    "new_itos = [re.sub(' ','_',i) for i in new_itos]\n",
    "#replace \\n with xxeol\n",
    "new_itos[4] = 'xxeol'\n",
    "thai2vec.index = new_itos\n",
    "thai2vec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ː]</th>\n",
       "      <td>4.526951</td>\n",
       "      <td>-0.684044</td>\n",
       "      <td>0.658317</td>\n",
       "      <td>-34.973621</td>\n",
       "      <td>32.692394</td>\n",
       "      <td>-21.356636</td>\n",
       "      <td>-0.297866</td>\n",
       "      <td>13.243143</td>\n",
       "      <td>33.953781</td>\n",
       "      <td>-2.175802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913986</td>\n",
       "      <td>-20.240484</td>\n",
       "      <td>31.480227</td>\n",
       "      <td>6.308282</td>\n",
       "      <td>0.222549</td>\n",
       "      <td>-12.421679</td>\n",
       "      <td>-5.094245</td>\n",
       "      <td>-0.876305</td>\n",
       "      <td>-16.382822</td>\n",
       "      <td>-1.946360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>femina</th>\n",
       "      <td>5.643933</td>\n",
       "      <td>-0.454897</td>\n",
       "      <td>-0.781048</td>\n",
       "      <td>-34.636936</td>\n",
       "      <td>32.502296</td>\n",
       "      <td>-23.502565</td>\n",
       "      <td>0.150455</td>\n",
       "      <td>13.833745</td>\n",
       "      <td>35.022408</td>\n",
       "      <td>-0.543715</td>\n",
       "      <td>...</td>\n",
       "      <td>1.087782</td>\n",
       "      <td>-22.103447</td>\n",
       "      <td>33.913628</td>\n",
       "      <td>7.328459</td>\n",
       "      <td>0.590396</td>\n",
       "      <td>-11.619066</td>\n",
       "      <td>-6.422720</td>\n",
       "      <td>-0.582779</td>\n",
       "      <td>-15.718392</td>\n",
       "      <td>-1.168420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>แมชอัป</th>\n",
       "      <td>4.505654</td>\n",
       "      <td>0.813675</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>-36.005527</td>\n",
       "      <td>33.578327</td>\n",
       "      <td>-22.218058</td>\n",
       "      <td>-0.812224</td>\n",
       "      <td>14.030306</td>\n",
       "      <td>33.559189</td>\n",
       "      <td>-1.831509</td>\n",
       "      <td>...</td>\n",
       "      <td>2.220871</td>\n",
       "      <td>-20.002760</td>\n",
       "      <td>31.140167</td>\n",
       "      <td>7.040421</td>\n",
       "      <td>0.533256</td>\n",
       "      <td>-12.791709</td>\n",
       "      <td>-2.348727</td>\n",
       "      <td>-1.542387</td>\n",
       "      <td>-12.926419</td>\n",
       "      <td>-0.873332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko-d</th>\n",
       "      <td>4.165557</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>-0.987677</td>\n",
       "      <td>-35.324249</td>\n",
       "      <td>32.025230</td>\n",
       "      <td>-20.510134</td>\n",
       "      <td>-0.564647</td>\n",
       "      <td>14.144468</td>\n",
       "      <td>33.327763</td>\n",
       "      <td>-0.892376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502303</td>\n",
       "      <td>-20.634552</td>\n",
       "      <td>31.940294</td>\n",
       "      <td>7.187971</td>\n",
       "      <td>-0.247172</td>\n",
       "      <td>-11.427867</td>\n",
       "      <td>-5.017011</td>\n",
       "      <td>-0.749183</td>\n",
       "      <td>-15.432819</td>\n",
       "      <td>-0.678519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>­่</th>\n",
       "      <td>5.919821</td>\n",
       "      <td>-0.623172</td>\n",
       "      <td>-1.006667</td>\n",
       "      <td>-37.552708</td>\n",
       "      <td>34.542671</td>\n",
       "      <td>-22.609804</td>\n",
       "      <td>-0.612308</td>\n",
       "      <td>13.414227</td>\n",
       "      <td>36.015945</td>\n",
       "      <td>-0.484734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972129</td>\n",
       "      <td>-21.235813</td>\n",
       "      <td>32.551910</td>\n",
       "      <td>7.112097</td>\n",
       "      <td>1.700822</td>\n",
       "      <td>-14.420567</td>\n",
       "      <td>-7.045519</td>\n",
       "      <td>-0.921345</td>\n",
       "      <td>-16.223467</td>\n",
       "      <td>-0.626158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>มูโญซ</th>\n",
       "      <td>3.681906</td>\n",
       "      <td>-0.218666</td>\n",
       "      <td>0.948459</td>\n",
       "      <td>-33.641006</td>\n",
       "      <td>31.695154</td>\n",
       "      <td>-22.561327</td>\n",
       "      <td>0.889483</td>\n",
       "      <td>13.910150</td>\n",
       "      <td>34.946846</td>\n",
       "      <td>-1.855071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924642</td>\n",
       "      <td>-21.629454</td>\n",
       "      <td>29.379236</td>\n",
       "      <td>7.132369</td>\n",
       "      <td>0.257729</td>\n",
       "      <td>-12.290989</td>\n",
       "      <td>-5.269632</td>\n",
       "      <td>-1.540430</td>\n",
       "      <td>-15.146802</td>\n",
       "      <td>-0.754419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>สัมภว</th>\n",
       "      <td>5.752477</td>\n",
       "      <td>0.315618</td>\n",
       "      <td>0.293014</td>\n",
       "      <td>-36.698799</td>\n",
       "      <td>34.379864</td>\n",
       "      <td>-23.329159</td>\n",
       "      <td>-0.070712</td>\n",
       "      <td>13.357928</td>\n",
       "      <td>35.762405</td>\n",
       "      <td>-1.047630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906964</td>\n",
       "      <td>-21.077778</td>\n",
       "      <td>33.069584</td>\n",
       "      <td>7.098242</td>\n",
       "      <td>1.803606</td>\n",
       "      <td>-11.642494</td>\n",
       "      <td>-5.033253</td>\n",
       "      <td>0.524389</td>\n",
       "      <td>-16.313740</td>\n",
       "      <td>-1.029239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>เซ็ตเบ</th>\n",
       "      <td>4.389275</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>-1.411201</td>\n",
       "      <td>-36.771015</td>\n",
       "      <td>34.442936</td>\n",
       "      <td>-23.039188</td>\n",
       "      <td>1.612276</td>\n",
       "      <td>14.783897</td>\n",
       "      <td>35.015724</td>\n",
       "      <td>-1.216834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590751</td>\n",
       "      <td>-19.663397</td>\n",
       "      <td>30.306244</td>\n",
       "      <td>6.418957</td>\n",
       "      <td>-0.314835</td>\n",
       "      <td>-10.774155</td>\n",
       "      <td>-4.693771</td>\n",
       "      <td>-0.300360</td>\n",
       "      <td>-12.498419</td>\n",
       "      <td>-1.852149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dacc</th>\n",
       "      <td>4.742739</td>\n",
       "      <td>-0.908443</td>\n",
       "      <td>0.547335</td>\n",
       "      <td>-35.256859</td>\n",
       "      <td>31.603384</td>\n",
       "      <td>-24.493853</td>\n",
       "      <td>-0.259395</td>\n",
       "      <td>13.682415</td>\n",
       "      <td>35.604534</td>\n",
       "      <td>-1.805019</td>\n",
       "      <td>...</td>\n",
       "      <td>1.418943</td>\n",
       "      <td>-22.847486</td>\n",
       "      <td>32.852303</td>\n",
       "      <td>7.006009</td>\n",
       "      <td>-0.039930</td>\n",
       "      <td>-10.819450</td>\n",
       "      <td>-6.981693</td>\n",
       "      <td>-1.151021</td>\n",
       "      <td>-14.195145</td>\n",
       "      <td>-1.810433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monsta</th>\n",
       "      <td>5.333290</td>\n",
       "      <td>0.036935</td>\n",
       "      <td>0.209927</td>\n",
       "      <td>-38.188976</td>\n",
       "      <td>33.112858</td>\n",
       "      <td>-21.396271</td>\n",
       "      <td>-0.456774</td>\n",
       "      <td>13.161014</td>\n",
       "      <td>34.834602</td>\n",
       "      <td>-1.270794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637157</td>\n",
       "      <td>-20.428938</td>\n",
       "      <td>33.175938</td>\n",
       "      <td>7.091130</td>\n",
       "      <td>1.161157</td>\n",
       "      <td>-11.597325</td>\n",
       "      <td>-5.889217</td>\n",
       "      <td>-0.936701</td>\n",
       "      <td>-16.721420</td>\n",
       "      <td>-1.459496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2          3          4          5    \\\n",
       "ː]      4.526951 -0.684044  0.658317 -34.973621  32.692394 -21.356636   \n",
       "femina  5.643933 -0.454897 -0.781048 -34.636936  32.502296 -23.502565   \n",
       "แมชอัป  4.505654  0.813675  0.541200 -36.005527  33.578327 -22.218058   \n",
       "ko-d    4.165557  0.011141 -0.987677 -35.324249  32.025230 -20.510134   \n",
       "­่      5.919821 -0.623172 -1.006667 -37.552708  34.542671 -22.609804   \n",
       "มูโญซ   3.681906 -0.218666  0.948459 -33.641006  31.695154 -22.561327   \n",
       "สัมภว   5.752477  0.315618  0.293014 -36.698799  34.379864 -23.329159   \n",
       "เซ็ตเบ  4.389275  0.097917 -1.411201 -36.771015  34.442936 -23.039188   \n",
       "dacc    4.742739 -0.908443  0.547335 -35.256859  31.603384 -24.493853   \n",
       "monsta  5.333290  0.036935  0.209927 -38.188976  33.112858 -21.396271   \n",
       "\n",
       "             6          7          8         9      ...          390  \\\n",
       "ː]     -0.297866  13.243143  33.953781 -2.175802    ...     0.913986   \n",
       "femina  0.150455  13.833745  35.022408 -0.543715    ...     1.087782   \n",
       "แมชอัป -0.812224  14.030306  33.559189 -1.831509    ...     2.220871   \n",
       "ko-d   -0.564647  14.144468  33.327763 -0.892376    ...     1.502303   \n",
       "­่     -0.612308  13.414227  36.015945 -0.484734    ...     0.972129   \n",
       "มูโญซ   0.889483  13.910150  34.946846 -1.855071    ...     1.924642   \n",
       "สัมภว  -0.070712  13.357928  35.762405 -1.047630    ...     0.906964   \n",
       "เซ็ตเบ  1.612276  14.783897  35.015724 -1.216834    ...     0.590751   \n",
       "dacc   -0.259395  13.682415  35.604534 -1.805019    ...     1.418943   \n",
       "monsta -0.456774  13.161014  34.834602 -1.270794    ...     0.637157   \n",
       "\n",
       "              391        392       393       394        395       396  \\\n",
       "ː]     -20.240484  31.480227  6.308282  0.222549 -12.421679 -5.094245   \n",
       "femina -22.103447  33.913628  7.328459  0.590396 -11.619066 -6.422720   \n",
       "แมชอัป -20.002760  31.140167  7.040421  0.533256 -12.791709 -2.348727   \n",
       "ko-d   -20.634552  31.940294  7.187971 -0.247172 -11.427867 -5.017011   \n",
       "­่     -21.235813  32.551910  7.112097  1.700822 -14.420567 -7.045519   \n",
       "มูโญซ  -21.629454  29.379236  7.132369  0.257729 -12.290989 -5.269632   \n",
       "สัมภว  -21.077778  33.069584  7.098242  1.803606 -11.642494 -5.033253   \n",
       "เซ็ตเบ -19.663397  30.306244  6.418957 -0.314835 -10.774155 -4.693771   \n",
       "dacc   -22.847486  32.852303  7.006009 -0.039930 -10.819450 -6.981693   \n",
       "monsta -20.428938  33.175938  7.091130  1.161157 -11.597325 -5.889217   \n",
       "\n",
       "             397        398       399  \n",
       "ː]     -0.876305 -16.382822 -1.946360  \n",
       "femina -0.582779 -15.718392 -1.168420  \n",
       "แมชอัป -1.542387 -12.926419 -0.873332  \n",
       "ko-d   -0.749183 -15.432819 -0.678519  \n",
       "­่     -0.921345 -16.223467 -0.626158  \n",
       "มูโญซ  -1.540430 -15.146802 -0.754419  \n",
       "สัมภว   0.524389 -16.313740 -1.029239  \n",
       "เซ็ตเบ -0.300360 -12.498419 -1.852149  \n",
       "dacc   -1.151021 -14.195145 -1.810433  \n",
       "monsta -0.936701 -16.721420 -1.459496  \n",
       "\n",
       "[10 rows x 400 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai2vec.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:55:23.616554Z",
     "start_time": "2018-01-25T03:55:03.872457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 400)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thai2save = thai2vec \n",
    "thai2save.to_csv(f'{MODEL_PATH}thai2vec.vec',sep=' ',header=False, line_terminator='\\n')\n",
    "#add NB_ROWS NB_COLS as header\n",
    "thai2save.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(f'{MODEL_PATH}thai2vec.vec',binary=False,\n",
    "                                         unicode_errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_word2vec_format(f'{MODEL_PATH}thai2vec.vec',f'{MODEL_PATH}thai2vec.vocab',False)\n",
    "# model.save_word2vec_format(f'{MODEL_PATH}thai2vec.bin',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get document vector from the language model by applying the encoder to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = ThaiTokenizer()\n",
    "def document_vector(ss, learn, data):\n",
    "    s = tt.tokenizer(ss)\n",
    "    t = torch.tensor(data.vocab.numericalize(s), requires_grad=False)[:,None].to(device)\n",
    "    m = learn.model[0]\n",
    "    m.reset()\n",
    "    pred,_ = m(t)\n",
    "    res = pred[-1][-1,:,:].squeeze().detach().numpy()\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'วันนี้วันดีปีใหม่'\n",
    "document_vector(ss,learn,data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.ulmfit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.083069,  0.014881,  0.010677,  0.012863, ..., -0.246675, -0.015147,  0.153276, -0.004983], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector(ss,learn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
